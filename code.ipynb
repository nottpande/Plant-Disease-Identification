{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aditya Pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'training1/.cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path, save_weights_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('accuracy') > 0.99:\n",
    "            print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '5'> ADDING THE IMAGES TO TENSORFLOW DATASETS (TENSORFLOW INPUT PIPELINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> Creating constant for items that will be used a lot in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "batch_size = 64\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> Adding files to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20638 files belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Data\",\n",
    "    shuffle=True,\n",
    "    image_size=(img_size,img_size),\n",
    "    batch_size= batch_size,\n",
    "    label_mode='int'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(dataset.class_names)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> This returns a PrefetchDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'> The folders present in the 'Data'  folder are basically all the classes that are present here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '5'> SPLITTING THE DATA INTO TRAIN AND TEST SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> We will split the training set into Train and test set, where the training set is 80% of the dataset and then the remaining 20% is used as 10% for testing the model, and 10% for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'> We will split the dataset using the '.skip' and the '.take' functions present in the tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set = 258 elements\n"
     ]
    }
   ],
   "source": [
    "train_len = int(0.8 * len(dataset))\n",
    "train_set = dataset.take(train_len)\n",
    "verify_set = dataset.skip(train_len)\n",
    "print(f\"Length of training set = {len(train_set)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> Furthur splitting the 'test_set' into test_set and validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set = 32 elements\n",
      "Length of validation set = 33 elements\n"
     ]
    }
   ],
   "source": [
    "test_len = int(0.1*len(dataset))\n",
    "test_set = verify_set.take(test_len)\n",
    "validation_set = verify_set.skip(test_len)\n",
    "print(f\"Length of test set = {len(test_set)} elements\")\n",
    "print(f\"Length of validation set = {len(validation_set)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> Now we have our train_set, test_set and validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> Now we will use the '.prefetch' method, because The .prefetch method is used to overlap the data loading and model training. It prefetches batches of data in the background while the model is executing training steps, which makes the program solving faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size ='3'>The '.cache' method is used to cache data, which can significantly improve the data loading speed. We have done mapping before the caching process because:\n",
    "Caching is done to store the preprocessed data in memory or on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_set = test_set.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_set = validation_set.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '5'> DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> Here we will build layers for the preprocessing that has to be done to the images such as Scaling, Resizing and Data Augmentation before the images are passed as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aditya Pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescale_and_resize = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.Resizing(img_size,img_size), #this will make sure that all the images are in the required scale\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1.0/255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '5'> BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> First we will add the rescale, resize and data augmentation layers in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> Now we will add the CNN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> Creating a stack of convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aditya Pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    rescale_and_resize,\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3),padding='valid'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu',padding='valid'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu',padding='valid'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu',padding='valid'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu',padding='valid'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu',padding='valid'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=n_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aditya Pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 2, 2, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 184527 (720.81 KB)\n",
      "Trainable params: 184527 (720.81 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(None,img_size,img_size,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:From C:\\Users\\Aditya Pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Aditya Pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "258/258 [==============================] - ETA: 0s - loss: 2.0914 - accuracy: 0.3242\n",
      "Epoch 1: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 662s 2s/step - loss: 2.0914 - accuracy: 0.3242 - val_loss: 1.6651 - val_accuracy: 0.4591\n",
      "Epoch 2/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 1.2660 - accuracy: 0.5796\n",
      "Epoch 2: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 578s 2s/step - loss: 1.2660 - accuracy: 0.5796 - val_loss: 1.1828 - val_accuracy: 0.6044\n",
      "Epoch 3/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.8986 - accuracy: 0.6949\n",
      "Epoch 3: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 619s 2s/step - loss: 0.8986 - accuracy: 0.6949 - val_loss: 0.7964 - val_accuracy: 0.7267\n",
      "Epoch 4/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7253 - accuracy: 0.7482\n",
      "Epoch 4: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 591s 2s/step - loss: 0.7253 - accuracy: 0.7482 - val_loss: 1.1487 - val_accuracy: 0.6347\n",
      "Epoch 5/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.6303 - accuracy: 0.7822\n",
      "Epoch 5: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 599s 2s/step - loss: 0.6303 - accuracy: 0.7822 - val_loss: 0.7668 - val_accuracy: 0.7440\n",
      "Epoch 6/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.8095\n",
      "Epoch 6: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 608s 2s/step - loss: 0.5639 - accuracy: 0.8095 - val_loss: 0.9105 - val_accuracy: 0.7151\n",
      "Epoch 7/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.8396\n",
      "Epoch 7: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 580s 2s/step - loss: 0.4558 - accuracy: 0.8396 - val_loss: 0.6379 - val_accuracy: 0.7936\n",
      "Epoch 8/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.4158 - accuracy: 0.8563\n",
      "Epoch 8: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 582s 2s/step - loss: 0.4158 - accuracy: 0.8563 - val_loss: 0.6457 - val_accuracy: 0.7878\n",
      "Epoch 9/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8663\n",
      "Epoch 9: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 580s 2s/step - loss: 0.3894 - accuracy: 0.8663 - val_loss: 0.6742 - val_accuracy: 0.7806\n",
      "Epoch 10/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8809\n",
      "Epoch 10: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 587s 2s/step - loss: 0.3443 - accuracy: 0.8809 - val_loss: 0.7974 - val_accuracy: 0.7517\n",
      "Epoch 11/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.8820\n",
      "Epoch 11: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 587s 2s/step - loss: 0.3384 - accuracy: 0.8820 - val_loss: 0.5673 - val_accuracy: 0.8248\n",
      "Epoch 12/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8932\n",
      "Epoch 12: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 594s 2s/step - loss: 0.3094 - accuracy: 0.8932 - val_loss: 0.7985 - val_accuracy: 0.7589\n",
      "Epoch 13/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.8994\n",
      "Epoch 13: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 594s 2s/step - loss: 0.2906 - accuracy: 0.8994 - val_loss: 0.3423 - val_accuracy: 0.8826\n",
      "Epoch 14/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9074\n",
      "Epoch 14: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 614s 2s/step - loss: 0.2628 - accuracy: 0.9074 - val_loss: 0.6207 - val_accuracy: 0.8027\n",
      "Epoch 15/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.9103\n",
      "Epoch 15: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 598s 2s/step - loss: 0.2570 - accuracy: 0.9103 - val_loss: 0.4871 - val_accuracy: 0.8397\n",
      "Epoch 16/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.9169\n",
      "Epoch 16: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 605s 2s/step - loss: 0.2438 - accuracy: 0.9169 - val_loss: 0.4639 - val_accuracy: 0.8561\n",
      "Epoch 17/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9170\n",
      "Epoch 17: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 616s 2s/step - loss: 0.2403 - accuracy: 0.9170 - val_loss: 0.4246 - val_accuracy: 0.8638\n",
      "Epoch 18/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9190\n",
      "Epoch 18: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 637s 2s/step - loss: 0.2466 - accuracy: 0.9190 - val_loss: 0.3275 - val_accuracy: 0.8893\n",
      "Epoch 19/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9274\n",
      "Epoch 19: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 663s 3s/step - loss: 0.2075 - accuracy: 0.9274 - val_loss: 0.2563 - val_accuracy: 0.9081\n",
      "Epoch 20/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9311\n",
      "Epoch 20: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 719s 3s/step - loss: 0.2007 - accuracy: 0.9311 - val_loss: 0.4953 - val_accuracy: 0.8436\n",
      "Epoch 21/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1884 - accuracy: 0.9327\n",
      "Epoch 21: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 625s 2s/step - loss: 0.1884 - accuracy: 0.9327 - val_loss: 0.3101 - val_accuracy: 0.8956\n",
      "Epoch 22/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1829 - accuracy: 0.9389\n",
      "Epoch 22: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 672s 3s/step - loss: 0.1829 - accuracy: 0.9389 - val_loss: 0.2662 - val_accuracy: 0.9076\n",
      "Epoch 23/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9361\n",
      "Epoch 23: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 707s 3s/step - loss: 0.1838 - accuracy: 0.9361 - val_loss: 0.3284 - val_accuracy: 0.8874\n",
      "Epoch 24/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9394\n",
      "Epoch 24: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 700s 3s/step - loss: 0.1775 - accuracy: 0.9394 - val_loss: 0.2884 - val_accuracy: 0.9052\n",
      "Epoch 25/25\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1578 - accuracy: 0.9459\n",
      "Epoch 25: saving model to training1\\.cp.ckpt\n",
      "258/258 [==============================] - 774s 3s/step - loss: 0.1578 - accuracy: 0.9459 - val_loss: 0.1869 - val_accuracy: 0.9293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b8dee0c7d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_set,epochs=epochs,batch_size=batch_size,validation_data=validation_set,callbacks=[callbacks,cp_callback],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 35s 551ms/step - loss: 0.2168 - accuracy: 0.9248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2167801856994629, 0.9248046875]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Versions/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Versions/1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('/Versions/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
